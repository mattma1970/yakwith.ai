{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch; torch.version.cuda\n",
    "import torch; torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --upgrade --force-reinstall --no-cache-dir torch==2.1.0 triton --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install --upgrade pip\n",
    "!pip install --no-deps --no-build-isolation packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install \"unsloth[cu121-ampere] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtman/Documents/Repos/yakwith.ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4000 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.22.post7. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/mtman/Documents/Repos/yakwith.ai/models/Mistral-7B-OpenOrca\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "def format_text(text):\n",
    "    return {'text':str(text)}\n",
    "dataset = load_dataset('csv', data_files='/home/mtman/Documents/Repos/yakwith.ai/data/1711405462.204613_881/function_call_data.tsv', split='train')\n",
    "dataset = dataset.map(format_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"<|im_start|>system\\\\nYou are a clever conversation analyst. You are given a conversation between a waiter (the \\'assistant\\') and a customer (the \\'user\\') at a restaurant. Your TASK is to generate three parameters required by a FUNCTION that keeps track of the current state of the customer\\'s ORDER. To understand the context of the customers final request, you will need to review the conversation history. The parameters must be returned in JSON format using the following schema:\\\\n                     {\\'item\\': the name of the current menu_item being discussed,\\\\n                      \\'action\\': how the ORDER should be changed: \\'add\\',\\'remove\\',\\'update\\',\\\\n                      \\'variation\\': the choice or modification that the customer has asked for. If they have not made any choices then this value should be \\'none\\'\\\\n                      }<|im_end|>\\\\n<|im_start|>user\\\\nHello, can I see the menu for the cocktails?<|im_end|>\\\\n<|im_start|>analyst\\\\n[{\\'action\\': \\'none\\', \\'menu_item\\': \\'none\\', \\'variation\\': \\'none\\'}]<|im_end|>\\\\n\": \"<|im_start|>system\\\\nYou are a clever conversation analyst. You are given a conversation between a waiter (the \\'assistant\\') and a customer (the \\'user\\') at a restaurant. Your TASK is to generate three parameters required by a FUNCTION that keeps track of the current state of the customer\\'s ORDER. To understand the context of the customers final request, you will need to review the conversation history. The parameters must be returned in JSON format using the following schema:\\\\n                     {\\'item\\': the name of the current menu_item being discussed,\\\\n                      \\'action\\': how the ORDER should be changed: \\'add\\',\\'remove\\',\\'update\\',\\\\n                      \\'variation\\': the choice or modification that the customer has asked for. If they have not made any choices then this value should be \\'none\\'\\\\n                      }<|im_end|>\\\\n<|im_start|>user\\\\nHello, can I see the menu for the cocktails?<|im_end|>\\\\n<|im_start|>assistant\\\\nOf course! We have APEROL SPRITZ, MARGARITA, and ESPRESSO MARTINI. Would you like to order any of these?<|im_end|>\\\\n<|im_start|>user\\\\nI\\'ll have an ESPRESSO MARTINI, please.<|im_end|>\\\\n<|im_start|>analyst\\\\n[{\\'action\\': \\'add\\', \\'menu_item\\': \\'ESPRESSO MARTINI\\', \\'variation\\': \\'none\\'}]<|im_end|>\\\\n\"}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset); dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 5,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 25,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 83 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 5 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 5 | Total steps = 25\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:24, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.770100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.543300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\":\"system\",'content':'{\"\"food_items\"\": [{\"\"name\"\": \"\"EGGS POACHED OR FRIED\"\", \"\"price\"\": \"\"$13\"\", \"\"descriptions\"\": [\"\"Served on Sourdough or Ciabatta\"\"], \"\"other\"\": [\"\"Add Cheese - $1, Smashed Avo - $2\"\"]}, {\"\"name\"\": \"\"RUSTIC TOAST\"\", \"\"price\"\": \"\"$8.50\"\", \"\"descriptions\"\": [\"\"Sourdough or Ciabatta\"\", \"\"Fig & Lemon Jam\"\", \"\"Orange Marmalade\"\", \"\"Strawberry Jam\"\", \"\"Peanut Butter or Vegemite\"\"]}, {\"\"name\"\": \"\"MEXICAN eggs\"\", \"\"price\"\": \"\"$23\"\", \"\"descriptions\"\": [\"\"Mixed Beans Cooked with Chili, Garlic, Onion and Herbs in a Rich Tomato Sauce\"\", \"\"Baked with Mozzarella\"\", \"\"Two Poached Eggs\"\", \"\"Ciabatta\"\"]}, {\"\"name\"\": \"\"HOT CHEESE\"\", \"\"price\"\": \"\"$9.50\"\", \"\"descriptions\"\": [\"\"With Tomato & Oregano on Sourdough\"\"]}], \"\"tea\"\": {\"\"T-NOMICS ORGANIC LEAF TEA\"\": \"\"$5\"\", \"\"Varieties\"\": [\"\"English Breakfast\"\", \"\"Earl Grey\"\", \"\"Peppermint\"\", \"\"Lemongrass & Ginger\"\", \"\"Golden Oolong Green\"\"]}, \"\"immune_booster_shake\"\": {\"\"price\"\": \"\"$14\"\", \"\"Ingredients\"\": [\"\"Green apple\"\", \"\"Banana\"\", \"\"Spinach\"\", \"\"Mint\"\", \"\"Lime\"\", \"\"Ginger\"\"]}, \"\"protein_shake\"\": {\"\"price\"\": \"\"$14\"\", \"\"Ingredients\"\": [\"\"Peanut butter\"\", \"\"Modern pea protein\"\", \"\"Almond milk\"\", \"\"Banana\"\", \"\"Honey & ice\"\"], \"\"Additions\"\": [\"\"Add shot of coffee $0.5\"\"]}, \"\"extra_items\"\": {\"\"Salmon, Gourmet Pork Sausages\"\": \"\"$6\"\", \"\"Bacon, Haloumi\"\": \"\"$5\"\", \"\"Avocado, Grilled Tomato\"\": \"\"$4\"\", \"\"Free-Range Egg\"\": \"\"$3\"\", \"\"Gluten-Free Bread\"\": \"\"$2\"\"}}'},{\"role\":\"user\",\"content\":'For the Hot Cheese, can I have haloumi instead of the regular cheese?'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\":\"system\",'content':'{\"\"food_items\"\": [{\"\"name\"\": \"\"EGGS POACHED OR FRIED\"\", \"\"price\"\": \"\"$13\"\", \"\"descriptions\"\": [\"\"Served on Sourdough or Ciabatta\"\"], \"\"other\"\": [\"\"Add Cheese - $1, Smashed Avo - $2\"\"]}, {\"\"name\"\": \"\"RUSTIC TOAST\"\", \"\"price\"\": \"\"$8.50\"\", \"\"descriptions\"\": [\"\"Sourdough or Ciabatta\"\", \"\"Fig & Lemon Jam\"\", \"\"Orange Marmalade\"\", \"\"Strawberry Jam\"\", \"\"Peanut Butter or Vegemite\"\"]}, {\"\"name\"\": \"\"MEXICAN eggs\"\", \"\"price\"\": \"\"$23\"\", \"\"descriptions\"\": [\"\"Mixed Beans Cooked with Chili, Garlic, Onion and Herbs in a Rich Tomato Sauce\"\", \"\"Baked with Mozzarella\"\", \"\"Two Poached Eggs\"\", \"\"Ciabatta\"\"]}, {\"\"name\"\": \"\"HOT CHEESE\"\", \"\"price\"\": \"\"$9.50\"\", \"\"descriptions\"\": [\"\"With Tomato & Oregano on Sourdough\"\"]}], \"\"tea\"\": {\"\"T-NOMICS ORGANIC LEAF TEA\"\": \"\"$5\"\", \"\"Varieties\"\": [\"\"English Breakfast\"\", \"\"Earl Grey\"\", \"\"Peppermint\"\", \"\"Lemongrass & Ginger\"\", \"\"Golden Oolong Green\"\"]}, \"\"immune_booster_shake\"\": {\"\"price\"\": \"\"$14\"\", \"\"Ingredients\"\": [\"\"Green apple\"\", \"\"Banana\"\", \"\"Spinach\"\", \"\"Mint\"\", \"\"Lime\"\", \"\"Ginger\"\"]}, \"\"protein_shake\"\": {\"\"price\"\": \"\"$14\"\", \"\"Ingredients\"\": [\"\"Peanut butter\"\", \"\"Modern pea protein\"\", \"\"Almond milk\"\", \"\"Banana\"\", \"\"Honey & ice\"\"], \"\"Additions\"\": [\"\"Add shot of coffee $0.5\"\"]}, \"\"extra_items\"\": {\"\"Salmon, Gourmet Pork Sausages\"\": \"\"$6\"\", \"\"Bacon, Haloumi\"\": \"\"$5\"\", \"\"Avocado, Grilled Tomato\"\": \"\"$4\"\", \"\"Free-Range Egg\"\": \"\"$3\"\", \"\"Gluten-Free Bread\"\": \"\"$2\"\"}}'},{\"role\":\"user\",\"content\":'Hi, can I just get the rustic toast thanks?'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\":\"system\",'content':'{\"\"food_items\"\": [{\"\"name\"\": \"\"eggs poached or fried\"\", \"\"price\"\": \"\"$13\"\", \"\"descriptions\"\": [\"\"Served on Sourdough or Ciabatta\"\"], \"\"other\"\": [\"\"Add Cheese - $1, Smashed Avo - $2\"\"]}, {\"\"name\"\": \"\"RUSTIC TOAST\"\", \"\"price\"\": \"\"$8.50\"\", \"\"descriptions\"\": [\"\"Sourdough or Ciabatta\"\", \"\"Fig & Lemon Jam\"\", \"\"Orange Marmalade\"\", \"\"Strawberry Jam\"\", \"\"Peanut Butter or Vegemite\"\"]}, {\"\"name\"\": \"\"MEXICAN eggs\"\", \"\"price\"\": \"\"$23\"\", \"\"descriptions\"\": [\"\"Mixed Beans Cooked with Chili, Garlic, Onion and Herbs in a Rich Tomato Sauce\"\", \"\"Baked with Mozzarella\"\", \"\"Two Poached Eggs\"\", \"\"Ciabatta\"\"]}, {\"\"name\"\": \"\"HOT CHEESE\"\", \"\"price\"\": \"\"$9.50\"\", \"\"descriptions\"\": [\"\"With Tomato & Oregano on Sourdough\"\"]}], \"\"tea\"\": {\"\"T-NOMICS ORGANIC LEAF TEA\"\": \"\"$5\"\", \"\"Varieties\"\": [\"\"English Breakfast\"\", \"\"Earl Grey\"\", \"\"Peppermint\"\", \"\"Lemongrass & Ginger\"\", \"\"Golden Oolong Green\"\"]}, \"\"immune_booster_shake\"\": {\"\"price\"\": \"\"$14\"\", \"\"Ingredients\"\": [\"\"Green apple\"\", \"\"Banana\"\", \"\"Spinach\"\", \"\"Mint\"\", \"\"Lime\"\", \"\"Ginger\"\"]}, \"\"protein_shake\"\": {\"\"price\"\": \"\"$14\"\", \"\"Ingredients\"\": [\"\"Peanut butter\"\", \"\"Modern pea protein\"\", \"\"Almond milk\"\", \"\"Banana\"\", \"\"Honey & ice\"\"], \"\"Additions\"\": [\"\"Add shot of coffee $0.5\"\"]}, \"\"extra_items\"\": {\"\"Salmon, Gourmet Pork Sausages\"\": \"\"$6\"\", \"\"Bacon, Haloumi\"\": \"\"$5\"\", \"\"Avocado, Grilled Tomato\"\": \"\"$4\"\", \"\"Free-Range Egg\"\": \"\"$3\"\", \"\"Gluten-Free Bread\"\": \"\"$2\"\"}}'},{\"role\":\"user\",\"content\":'Which teas are organic?'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Function Call Messages **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\"role\":\"system\", \"content\":\"You are a clever conversation analyst. You are given a conversation between a waiter (the 'assistant') and a customer (the 'user') at a restaurant. You have 3 TASKS to complete. You must return the answers in json format with the following schema: {'item': TASK 1 - identify the menu item that is being currently discussed; 'action': TASK 2 - decide if the customers intention is to 'order' ('add') an item from the menu , 'remove' from the ORDER or 'update' an item in the ORDER, 'variation': TASK 3 - state the list of selections, changes or variations to ingredients or size that the customer is requesting}.\\nExample Response 1: {'item':'italian omlete','action':'update','variation':'['remove garlic', 'add chilli']'. If the customer refers to more than one menu item then repeat the 3 TASKS for each menu item.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[system_prompt,{\"role\":\"user\",\"content\":\"I'd like to get the big breakfast thanks?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [system_prompt, {\"role\":\"user\",\"content\":\"I'd like to get the big breakfast thanks?\"},{'role':'assistant','content':'Sure. Anything else?'},{\"role\":\"user\",\"content\":\"Can I get tofu instead of sausages on that\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [system_prompt, {\"role\":\"user\",\"content\":\"Can I get the big breakfast thanks?\"},{'role':'assistant','content':'Sure. Anything else?'},{\"role\":\"user\",\"content\":\"Can I get tofu instead of sausages on that\"},{'role':'assistant','content':'Okay. Anything else?'}, {\"role\":\"user\",\"content\":\"Actually, don't worry about the tofu. The sausages will be fine.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [system_prompt, {\"role\":\"user\",\"content\":\"Can I get the big breakfast thanks?\"},{'role':'assistant','content':'Sure. Anything else?'},{\"role\":\"user\",\"content\":\"Can I get tofu instead of sausages on that\"},{'role':'assistant','content':'Okay. Anything else?'}, {\"role\":\"user\",\"content\":\"Actually, don't worry about the tofu. The sausages will be fine.\"},{'role':'assistant','content':'Okay. Anything else?'},{'role':'assistant','content':\"Actually, forget the big breakfast. I'll just get a coffee instead.Sorry\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [a,b,c,d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|> system\n",
      "You are a clever conversation analyst. You are given a conversation between a waiter (the 'assistant') and a customer (the 'user') at a restaurant. You have 3 TASKS to complete. You must return the answers in json format with the following schema: {'item': TASK 1 - identify the menu item that is being currently discussed; 'action': TASK 2 - decide if the customers intention is to 'order' ('add') an item from the menu , 'remove' from the ORDER or 'update' an item in the ORDER, 'variation': TASK 3 - state the list of selections, changes or variations to ingredients or size that the customer is requesting}.\n",
      "Example Response 1: {'item':'italian omlete','action':'update','variation':'['remove garlic', 'add chilli']'. If the customer refers to more than one menu item then repeat the 3 TASKS for each menu item.<|im_end|><|im_start|> user\n",
      "I'd like to get the big breakfast thanks?<|im_end|><|im_start|> analyst\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'action': 'order', 'item': 'big breakfast', 'variation': []}]<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "<s><|im_start|> system\n",
      "You are a clever conversation analyst. You are given a conversation between a waiter (the 'assistant') and a customer (the 'user') at a restaurant. You have 3 TASKS to complete. You must return the answers in json format with the following schema: {'item': TASK 1 - identify the menu item that is being currently discussed; 'action': TASK 2 - decide if the customers intention is to 'order' ('add') an item from the menu , 'remove' from the ORDER or 'update' an item in the ORDER, 'variation': TASK 3 - state the list of selections, changes or variations to ingredients or size that the customer is requesting}.\n",
      "Example Response 1: {'item':'italian omlete','action':'update','variation':'['remove garlic', 'add chilli']'. If the customer refers to more than one menu item then repeat the 3 TASKS for each menu item.<|im_end|><|im_start|> user\n",
      "I'd like to get the big breakfast thanks?<|im_end|><|im_start|> assistant\n",
      "Sure. Anything else?<|im_end|><|im_start|> user\n",
      "Can I get tofu instead of sausages on that<|im_end|><|im_start|> analyst\n",
      "{'item': 'big breakfast', 'action': 'update', 'variation': ['remove sausages', 'add tofu']<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "<s><|im_start|> system\n",
      "You are a clever conversation analyst. You are given a conversation between a waiter (the 'assistant') and a customer (the 'user') at a restaurant. You have 3 TASKS to complete. You must return the answers in json format with the following schema: {'item': TASK 1 - identify the menu item that is being currently discussed; 'action': TASK 2 - decide if the customers intention is to 'order' ('add') an item from the menu , 'remove' from the ORDER or 'update' an item in the ORDER, 'variation': TASK 3 - state the list of selections, changes or variations to ingredients or size that the customer is requesting}.\n",
      "Example Response 1: {'item':'italian omlete','action':'update','variation':'['remove garlic', 'add chilli']'. If the customer refers to more than one menu item then repeat the 3 TASKS for each menu item.<|im_end|><|im_start|> user\n",
      "Can I get the big breakfast thanks?<|im_end|><|im_start|> assistant\n",
      "Sure. Anything else?<|im_end|><|im_start|> user\n",
      "Can I get tofu instead of sausages on that<|im_end|><|im_start|> assistant\n",
      "Okay. Anything else?<|im_end|><|im_start|> user\n",
      "Actually, don't worry about the tofu. The sausages will be fine.<|im_end|><|im_start|> analyst\n",
      "{'action': 'update', 'variation': '['remove tofu', 'add sausages']'}<|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "<s><|im_start|> system\n",
      "You are a clever conversation analyst. You are given a conversation between a waiter (the 'assistant') and a customer (the 'user') at a restaurant. You have 3 TASKS to complete. You must return the answers in json format with the following schema: {'item': TASK 1 - identify the menu item that is being currently discussed; 'action': TASK 2 - decide if the customers intention is to 'order' ('add') an item from the menu , 'remove' from the ORDER or 'update' an item in the ORDER, 'variation': TASK 3 - state the list of selections, changes or variations to ingredients or size that the customer is requesting}.\n",
      "Example Response 1: {'item':'italian omlete','action':'update','variation':'['remove garlic', 'add chilli']'. If the customer refers to more than one menu item then repeat the 3 TASKS for each menu item.<|im_end|><|im_start|> user\n",
      "Can I get the big breakfast thanks?<|im_end|><|im_start|> assistant\n",
      "Sure. Anything else?<|im_end|><|im_start|> user\n",
      "Can I get tofu instead of sausages on that<|im_end|><|im_start|> assistant\n",
      "Okay. Anything else?<|im_end|><|im_start|> user\n",
      "Actually, don't worry about the tofu. The sausages will be fine.<|im_end|><|im_start|> assistant\n",
      "Okay. Anything else?<|im_end|><|im_start|> assistant\n",
      "Actually, forget the big breakfast. I'll just get a coffee instead.Sorry<|im_end|><|im_start|> analyst\n",
      "{'action': 'remove', 'item': 'big breakfast', 'variation': []}<|im_end|>\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(messages)):\n",
    "    inputs = tokenizer.apply_chat_template(messages[i],tokenize=False, add_generation_prompt=False)\n",
    "    inputs = inputs+'<|im_start|>analyst\\n'\n",
    "    inputs = tokenizer(inputs, padding=True, return_tensors='pt')\n",
    "    input = inputs['input_ids'].to('cuda')\n",
    "    _ = model.generate(input_ids = input, streamer=text_streamer, max_new_tokens = 256, use_cache = True, temperature = 0.01 )\n",
    "    print ('==================================')\n",
    "\n",
    "# tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 40.75 out of 61.94 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 112.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"/home/mtman/Documents/Repos/yakwith.ai/models/OrderManager-Mistral-7B-OpenOrca\", tokenizer, save_method=\"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
