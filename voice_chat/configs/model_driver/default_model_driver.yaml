name: OpenOrcaLocal
task: text_generation
url: http://localhost:8080 # base url for LLM server endpoint
model: none  # name of model. Might be a HF repo or docker mounted folder where model files are located ( e.g. /data/mistral-7b-instruct) where /data/ if the volume mount point used by docker.
pretrained_tokenizer: models/Mistral-7B-OpenOrca
token: DUMMY
timeout: null
headers:
  Content-Type: application/json
cookies: null
stream: True
stream_chunk_size: 5
params:
  stop_sequences: #Used only for text processing and does not affect text generation.
  - <|im_end|>
  - <|im_start|>
  max_length: 4096
  temperature: 0.4
  stream: ${..stream}