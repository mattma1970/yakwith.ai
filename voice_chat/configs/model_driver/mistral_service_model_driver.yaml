name: OpenOrcaLocal
task: text_generation
url: http://localhost:8080
model: none
pretrained_tokenizer: models/Mistral-7B-OpenOrca
token: DUMMY
timeout: null
headers:
  Content-Type: application/json
cookies: null
stream: True
stream_chunk_size: 5
params:
  stop_sequences:
  - <|im_end|>
  - <|im_start|>
  # max_length is absolute number of tokens, including the prompt, that can be handled. 
  # Avoid using max_new_tokens (= max_output_tokens-token_count(prompt))
  max_length: 2000
  temperature: 0.2
  stream: ${..stream}